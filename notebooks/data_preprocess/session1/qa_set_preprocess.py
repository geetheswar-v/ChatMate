# -*- coding: utf-8 -*-
"""qa_set_preprocess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bTptwQlJqqf1BjYol_hQw1dlsAzYWbJV
"""

from google.colab import files
uploaded = files.upload()

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences as Pad
import re
import numpy as np
import json

import pandas as pd
import io

df = pd.read_csv(io.BytesIO(uploaded['qa_set.csv']))

questions = df['questions'].to_list()
answers = df['answers'].to_list()

print(questions[10])
print(answers[10])

def clean_text(text):
    text = text.lower().strip()
    text = re.sub(r"i'm", "i am", text)
    text = re.sub(r"he's", "he is", text)
    text = re.sub(r"she's", "she is", text)
    text = re.sub(r"it's", "it is", text)
    text = re.sub(r"that's", "that is", text)
    text = re.sub(r"what's", "what is", text)
    text = re.sub(r"where's", "where is", text)
    text = re.sub(r"there's", "there is", text)
    text = re.sub(r"how's", "how is", text)
    text = re.sub(r'[" "]+', " ", text)
    text = re.sub(r"[-()\"#/@;:<>{}`+=~|.!?,]", "", text)
    
    return text

clean_questions = []
for question in questions: 
    clean_questions.append(clean_text(question))
    
clean_answers = []
for answer in answers:
    answer = 'startans ' + clean_text(answer) +' endans'
    clean_answers.append(answer)

print(clean_questions[10])
print(clean_answers[10])

print(len(clean_questions))
print(len(clean_answers))

minlen = 2
maxlen = 20
questions_temp = []
answers_temp = []

# filter the questions
for i,question in enumerate(clean_questions):
    if len(question.split()) <= maxlen and len(question.split()) >= minlen:
        questions_temp.append(question)
        answers_temp.append(clean_answers[i])
        
filtered_questions = []
filtered_answers = []

# filter the answers w.r.t qs
for i, answer in enumerate(answers_temp):
    if len(answer.split()) <= maxlen and len(answer.split()) >= minlen:
        filtered_answers.append(answer)
        filtered_questions.append(questions_temp[i])
        
print(len(filtered_answers))
print(len(filtered_questions))

print(filtered_questions[10])
print(filtered_answers[10])

vocabsize = 8000

# unk -> unknow vocab to replace not used vocabs
question_tokenizer = Tokenizer(num_words = vocabsize+1, oov_token = 'unk')
answer_tokenizer = Tokenizer(num_words = vocabsize+1,oov_token = 'unk')

# tokenize the questions and answers
question_tokenizer.fit_on_texts(filtered_questions)
answer_tokenizer.fit_on_texts(filtered_answers)

# build the input sequence and output sequence
q_sequences = question_tokenizer.texts_to_sequences(filtered_questions)
a_sequences = answer_tokenizer.texts_to_sequences(filtered_answers)

# pad sequences in same length so that we can train them in model
q_pad = Pad(q_sequences, padding = 'post')
a_pad = Pad(a_sequences, padding = 'post')

q_token_json = question_tokenizer.to_json()
a_token_json = answer_tokenizer.to_json()

with open('questions.json', 'w', encoding='utf-8') as f:
    f.write(json.dumps(q_token_json, ensure_ascii=False))
    f.close()

with open('answers.json', 'w', encoding='utf-8') as f:
    f.write(json.dumps(a_token_json, ensure_ascii=False))
    f.close()

np.savez('data.npz', q_pad, a_pad)

# save the data
from google.colab import files
files.download('questions.json')
files.download('answers.json')
files.download('data.npz')